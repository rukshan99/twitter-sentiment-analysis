{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "twitter-sentiment-analysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNMBt1NG/1utApUuEjycA2C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rukshan99/twitter-sentiment-analysis/blob/main/twitter_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connecting with Google Drive to get the data-set"
      ],
      "metadata": {
        "id": "NAXrk19ufOFY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZhLkmUkO59g"
      },
      "source": [
        "# Import PyDrive and associated libraries\n",
        "# This only needs to be done once per notebook\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "# This only needs to be done once per notebook\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download a file based on its file ID.\n",
        "\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "file_id = '1x4XtAigTb-fJ2TQJOCDXzAkVOsToBrCT' # Check your own ID in GDrive\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "\n",
        "# Save file in Colab memory\n",
        "downloaded.GetContentFile('tweet_data.csv')  "
      ],
      "metadata": {
        "id": "4sRUE9JSf7-g"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization"
      ],
      "metadata": {
        "id": "ifZWCqFjiyJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Regex package\n",
        "import re"
      ],
      "metadata": {
        "id": "OaW9_7wNi3F5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handle tweet features"
      ],
      "metadata": {
        "id": "3RCgWolepQqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example tweet\n",
        "tweet = \"RT @rukshaann I love this! üëç https://rukshanjayasekara.me #Portfolio #Fun\""
      ],
      "metadata": {
        "id": "LZlmLUhvjTmU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Retweet tag"
      ],
      "metadata": {
        "id": "dC8mJ6JXlTaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling the retweet(RT) tag\n",
        "# It is not required for analysing sentiment\n",
        "def replace_retweet(tweet, default_replace=\"\"):\n",
        "  tweet = re.sub('RT\\s+', default_replace, tweet)\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "Xfd-oGtSjxBV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Processed tweet: {}\".format(replace_retweet(tweet)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wa6j0z-OkYSN",
        "outputId": "bb113534-2d0f-4829-c7af-a749f8b19ebf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed tweet: @rukshaann I love this! üëç https://rukshanjayasekara.me #Portfolio #Fun\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### User tags"
      ],
      "metadata": {
        "id": "p7TXL1T7lb5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling user tags(@)\n",
        "# It is not required for analysing sentiment\n",
        "def replace_user(tweet, default_replace=\"user\"):\n",
        "  tweet = re.sub('\\B@\\w+', default_replace, tweet)\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "WLm7VWOFksu_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Processed tweet: {}\".format(replace_user(tweet)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtHO4awOlH61",
        "outputId": "df9c4ed5-e282-453a-f46d-b58ee60f6477"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed tweet: RT user I love this! üëç https://rukshanjayasekara.me #Portfolio #Fun\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Emojis"
      ],
      "metadata": {
        "id": "0sp5cCyRl6vE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install and import `emoji` package"
      ],
      "metadata": {
        "id": "md6QJljZnvpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install emoji --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9z7K9Z1gl-z6",
        "outputId": "656de541-eadd-4218-f792-d90da58ceb67"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-1.6.1.tar.gz (170 kB)\n",
            "\u001b[?25l\r\u001b[K     |‚ñà‚ñà                              | 10 kB 18.7 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñâ                            | 20 kB 23.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                          | 30 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        | 40 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 51 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 61 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 71 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 81 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 92 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170 kB 5.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.1-py3-none-any.whl size=169314 sha256=8d12b616387d7eb4450237de7fa9695a546cbdd13f73b3c4f10f2a00619301e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/5f/d3/03d313ddb3c2a1a427bb4690f1621eea60fe6f2a30cc95940f\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import emoji"
      ],
      "metadata": {
        "id": "sTc6DRvnn1D1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace emojis with meaningful text\n",
        "def demojize(tweet):\n",
        "  tweet = emoji.demojize(tweet)\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "bDqfaJq_mSZj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Processed tweet: {}\".format(demojize(tweet)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Xo7n7c3mddZ",
        "outputId": "0e7bc095-e0bf-4634-9162-c70e0785ec57"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed tweet: RT @rukshaann I love this! :thumbs_up: https://rukshanjayasekara.me #Portfolio #Fun\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### URLs"
      ],
      "metadata": {
        "id": "POwvRyGloHFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def replace_url(tweet, default_replace=\"\"):\n",
        "  tweet = re.sub('(http|https):\\/\\/\\S+', default_replace, tweet)\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "VOvb-vhgoLmO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Processed tweet: {}\".format(replace_url(tweet)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfDkTFBLoX_O",
        "outputId": "2419369b-52be-4bc2-aa50-0d788ba1404f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed tweet: RT @rukshaann I love this! üëç  #Portfolio #Fun\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hashtags"
      ],
      "metadata": {
        "id": "l624gn0cooLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove hashtag symbol(#)\n",
        "def replace_hashtag(tweet, default_replace=\"\"):\n",
        "  tweet = re.sub('#+', default_replace, tweet)\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "ghiatcKroqt9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Processed tweet: {}\".format(replace_hashtag(tweet)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdgrcG2Fov82",
        "outputId": "62f60156-80a7-44af-f096-3821b86d5c64"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed tweet: RT @rukshaann I love this! üëç https://rukshanjayasekara.me Portfolio Fun\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handle word features"
      ],
      "metadata": {
        "id": "epNmVOx0pcYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example tweet\n",
        "tweet = \"LOOOOOOOOK at this ... I'd like it so much!\""
      ],
      "metadata": {
        "id": "rPjJfMMl2D1a"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Capitals"
      ],
      "metadata": {
        "id": "bnPjAWAC2Wm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_lowercase(tweet):\n",
        "  tweet = tweet.lower()\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "XdObAbGq2P3h"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Processed tweet: {}\".format(to_lowercase(tweet)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLz0IRs42bmi",
        "outputId": "21d42eb0-aa68-4edc-939c-8ecf1b6aa6fb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed tweet: looooooook at this ... i'd like it so much!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Word repitions"
      ],
      "metadata": {
        "id": "OODO9fYb2nyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word_repetition(tweet):\n",
        "  tweet = re.sub(r'(.)\\1+', r'\\1\\1', tweet)\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "JzwfR06L2sSR"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Processed tweet: {}\".format(word_repetition(tweet)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuCaH1iV2zXp",
        "outputId": "f09be218-b3bf-4ac9-e295-f6681744eecb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed tweet: LOOK at this .. I'd like it so much!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Punctuation repetition"
      ],
      "metadata": {
        "id": "gqymZxIj2_gZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def punct_repetition(tweet, default_replace=\"\"):\n",
        "  tweet = re.sub(r'[\\?\\.\\!]+(?=[\\?\\.\\!])', default_replace, tweet)\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "1C149jMM3EDr"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Processed tweet: {}\".format(punct_repetition(tweet)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rutFMc5H3WcK",
        "outputId": "55fc0a05-aa45-46ea-a84b-f5cd8ec48bc8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed tweet: LOOOOOOOOK at this . I'd like it so much!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Word contraction"
      ],
      "metadata": {
        "id": "nessGAri3gIt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install and import the `contraction` package"
      ],
      "metadata": {
        "id": "biemaPtR3h2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21zPcgTl30B_",
        "outputId": "c0b3c75c-185b-42dd-9826-c9e5f22a538d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.0.58-py2.py3-none-any.whl (8.0 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 321 kB 5.7 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 284 kB 46.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85450 sha256=f3eeada9cddea4e29aeab25826c3bf37e2323536296a98f0a8dceffe3d2208eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.0 contractions-0.0.58 pyahocorasick-1.4.2 textsearch-0.0.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import contractions"
      ],
      "metadata": {
        "id": "JS6LvQxX33Wo"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace contractions with their extended forms\n",
        "def fix_contractions(tweet):\n",
        "  tweet = contractions.fix(tweet)\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "y2EBIZj34Bcg"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Processed tweet: {}\".format(fix_contractions(tweet)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52svUGU24J2X",
        "outputId": "723a42b1-2147-4271-deaf-917cea1ad063"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed tweet: LOOOOOOOOK at this ... I would like it so much!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization"
      ],
      "metadata": {
        "id": "BYXlhkFD4eUG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install and import the `NLTK` package"
      ],
      "metadata": {
        "id": "iLbuEPo24skW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmg8jeOU4lC1",
        "outputId": "d7864d3d-c5b8-4532-c5ff-fd5cd8f48871"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "TUq7oQAd4y5H"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the word_tokenize module from NLTK\n",
        "from nltk.tokenize import word_tokenize\n",
        "# Download the Punkt tokenizer model from NLTK\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrXfFU6f4-L3",
        "outputId": "4b3be29b-54a7-4b71-ea78-57e8b16087fa"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example tweet\n",
        "tweet = \"These are 5 different words!\""
      ],
      "metadata": {
        "id": "v57fBxUG5IBH"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns a tweet as a list of tokens\n",
        "def tokenize(tweet):\n",
        "  tokens = word_tokenize(tweet)\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "A178RI9p5S0v"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(tokenize(tweet)))\n",
        "print(\"Tweet tokens: {}\".format(tokenize(tweet)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PztAfMWe5eFo",
        "outputId": "d21f05e8-ebcb-4925-c011-a529f9b073c9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "Tweet tokens: ['These', 'are', '5', 'different', 'words', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Stopwords"
      ],
      "metadata": {
        "id": "81U8dagy6WfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the string package\n",
        "import string\n",
        "# Import the stopwords module from NLTK\n",
        "from nltk.corpus import stopwords\n",
        "# Download stopwords data from NLTK\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Euw4vLgc6upe",
        "outputId": "61229aee-79f1-4532-ce2a-3ccc7a2ab059"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "# Remove useful stopwords for sentiment analysis\n",
        "stop_words.discard('not')"
      ],
      "metadata": {
        "id": "gdcWrpvy7Etu"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjg5sAJx7KMl",
        "outputId": "70a4c7e4-77a8-4322-fe99-ed6965cf743b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a', 'her', 't', 'is', 'under', 'me', 'his', 'too', 'all', 'our', \"you've\", 'why', 'than', 'over', 'can', 'or', 'm', 'here', 'some', 'such', 'hers', 'the', 'more', 'mightn', 'but', 'wasn', \"should've\", 'on', 'y', 'by', 'wouldn', 'having', 'any', 'didn', \"you'll\", 'my', 'ourselves', 'other', 'who', 'which', 'off', 'be', 'myself', 'had', 'them', 'yourself', 'am', 'needn', 'and', 'for', 'to', 'there', 'through', 'shouldn', 'where', 'isn', 'just', \"wasn't\", \"mustn't\", 'down', 'are', 'whom', 'been', 'both', 'will', 'mustn', 'how', 'these', 'll', 'once', 'out', 'herself', 'nor', 's', 'does', 'have', 've', \"isn't\", 'this', \"it's\", \"doesn't\", 'itself', 'do', 'each', 'doesn', 'an', 'ours', \"needn't\", 'after', \"won't\", 'only', 'its', 'as', 'being', 'if', 'no', 'few', 'in', 'you', 'your', \"mightn't\", 'during', 'before', 'below', 'was', \"she's\", \"hadn't\", \"that'll\", \"wouldn't\", \"you'd\", 'about', 'ain', 'what', 'yours', 'from', \"shouldn't\", 'did', 'between', 'o', \"don't\", 'above', 'themselves', 'hadn', 'they', 'he', 'couldn', 'now', 'own', 'shan', \"couldn't\", 'don', 'while', \"weren't\", 'we', 'doing', 'at', 'him', 'when', 'again', 'theirs', 'into', 'up', 'until', 'then', 'haven', 'most', 'very', 'weren', 'against', 'i', \"didn't\", 'with', 'their', 'should', 'won', \"hasn't\", 'has', 'yourselves', 're', 'were', \"haven't\", 'hasn', 'd', 'further', \"aren't\", 'it', 'she', 'ma', 'because', 'same', \"you're\", 'so', \"shan't\", 'aren', 'those', 'that', 'of', 'himself'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update the `tokenize()` function to handle stopwords, punctuations, alphanumerics etc."
      ],
      "metadata": {
        "id": "KWfmhDc57yvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(tweet,\n",
        "             keep_punct = False,\n",
        "             keep_alnum = False,\n",
        "             keep_stop = False):\n",
        "  \n",
        "  tokens = word_tokenize(tweet)\n",
        "\n",
        "  if not keep_punct:\n",
        "    tokens = [token for token in tokens\n",
        "                  if token not in string.punctuation]\n",
        "\n",
        "  if not keep_alnum:\n",
        "    tokens = [token for token in tokens if token.isalpha()]\n",
        "  \n",
        "  if not keep_stop:\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stop_words.discard('not')\n",
        "    tokens = [token for token in tokens if not token in stop_words]\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "nEpCH9Aa794m"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tweet tokens: {}\".format(tokenize(tweet, \n",
        "                                                keep_punct=True, \n",
        "                                                keep_alnum=True, \n",
        "                                                keep_stop=True)))\n",
        "print(\"Tweet tokens: {}\".format(tokenize(tweet, keep_stop=True)))\n",
        "print(\"Tweet tokens: {}\".format(tokenize(tweet)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qUnqA5n9MY5",
        "outputId": "36c9df2f-1578-419a-939a-0c009da416c2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet tokens: ['These', 'are', '5', 'different', 'words', '!']\n",
            "Tweet tokens: ['These', 'are', 'different', 'words']\n",
            "Tweet tokens: ['These', 'different', 'words']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming"
      ],
      "metadata": {
        "id": "caR_65ou9qvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import different libraries and modules used for stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "metadata": {
        "id": "cwxhJSvP9tsA"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example token list\n",
        "tokens = [\"manager\", \"management\", \"managing\"]"
      ],
      "metadata": {
        "id": "7aHCqwca-DLb"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining different stemmers from NLTK package\n",
        "porter_stemmer = PorterStemmer()\n",
        "lancaster_stemmer = LancasterStemmer()\n",
        "snoball_stemmer = SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "FmQevP7Y-N7z"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns a stemmed token list\n",
        "# Should pass the stemmer a parameter\n",
        "def stem_tokens(tokens, stemmer):\n",
        "  token_list = []\n",
        "  for token in tokens:\n",
        "    token_list.append(stemmer.stem(token))\n",
        "  return token_list"
      ],
      "metadata": {
        "id": "QGc40QIu-Uki"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PorterStemmer\n",
        "print(\"Porter stems: {}\".format(stem_tokens(tokens, porter_stemmer)))\n",
        "# LancasterStemmer\n",
        "print(\"Lancaster stems: {}\".format(stem_tokens(tokens, lancaster_stemmer)))\n",
        "# SnowballStemmer\n",
        "print(\"Snowball stems: {}\".format(stem_tokens(tokens, snoball_stemmer)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3t9evag-otB",
        "outputId": "bc38c48d-4fc9-4a1f-c1bc-d5574b027847"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter stems: ['manag', 'manag', 'manag']\n",
            "Lancaster stems: ['man', 'man', 'man']\n",
            "Snowball stems: ['manag', 'manag', 'manag']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check over-stemming and under-stemming"
      ],
      "metadata": {
        "id": "OLTaxR0U_BS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [\"international\", \"companies\", \"had\", \"interns\"]\n",
        "\n",
        "print(\"Porter stems: {}\".format(stem_tokens(tokens, porter_stemmer)))\n",
        "print(\"Lancaster stems: {}\".format(stem_tokens(tokens, lancaster_stemmer)))\n",
        "print(\"Snowball stems: {}\".format(stem_tokens(tokens, snoball_stemmer)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Er1OxfiL_DIK",
        "outputId": "06e1a50c-c70f-44e0-bd50-c7f17d346041"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter stems: ['intern', 'compani', 'had', 'intern']\n",
            "Lancaster stems: ['intern', 'company', 'had', 'intern']\n",
            "Snowball stems: ['intern', 'compani', 'had', 'intern']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization"
      ],
      "metadata": {
        "id": "gWYGHkeC_Rxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import different libraries and modules used for lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jn_T0x3O_WqC",
        "outputId": "7dc78e41-71e1-4443-dd3c-436c29b5eabb"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example token list\n",
        "tokens = [\"international\", \"companies\", \"had\", \"interns\"]"
      ],
      "metadata": {
        "id": "C5R9aefC_qdz"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part of Speech (POS) tagging\n",
        "word_type = {\"international\": wordnet.ADJ, \n",
        "             \"companies\": wordnet.NOUN, \n",
        "             \"had\": wordnet.VERB, \n",
        "             \"interns\": wordnet.NOUN\n",
        "             }"
      ],
      "metadata": {
        "id": "soEOAjZZ_zzB"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the lemmatizer by using the WordNet module\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "WmxwJ-HW_7NB"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Takes the list of tokens as input and returns a list of lemmatized tokens\n",
        "def lemmatize_tokens(tokens, word_type, lemmatizer):\n",
        "  token_list = []\n",
        "  for token in tokens:\n",
        "    token_list.append(lemmatizer.lemmatize(token, word_type[token]))\n",
        "  return token_list"
      ],
      "metadata": {
        "id": "TvesPZDmAFmC"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tweet lemma: {}\".format(\n",
        "    lemmatize_tokens(tokens, word_type, lemmatizer)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIYhSmdwAMlJ",
        "outputId": "89f1bdac-e21b-4e39-befe-4b0fa2b0a66f"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet lemma: ['international', 'company', 'have', 'intern']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final normalizing function"
      ],
      "metadata": {
        "id": "WmokdVA6AlOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "complex_tweet = r\"\"\"RT @rukshaann : hey looooook, \n",
        "THis is a big and complex TWeet!!! üëç ... \n",
        "I'd be glad if you couldn't normalize it! \n",
        "Check https://rukshanjayasekara.me and LET ME KNOW!!! #NLP #Fun\"\"\""
      ],
      "metadata": {
        "id": "l12vtYT0AtZh"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_tweet(tweet, verbose=False):\n",
        "  if verbose: print(\"Initial tweet: {}\".format(tweet))\n",
        "\n",
        "  ## Twitter Features\n",
        "  tweet = replace_retweet(tweet) # replace retweet\n",
        "  tweet = replace_user(tweet, \"\") # replace user tag\n",
        "  tweet = replace_url(tweet) # replace url\n",
        "  tweet = replace_hashtag(tweet) # replace hashtag\n",
        "  if verbose: print(\"Post Twitter processing tweet: {}\".format(tweet))\n",
        "\n",
        "  ## Word Features\n",
        "  tweet = to_lowercase(tweet) # lower case\n",
        "  tweet = fix_contractions(tweet) # replace contractions\n",
        "  tweet = punct_repetition(tweet) # replace punctuation repetition\n",
        "  tweet = word_repetition(tweet) # replace word repetition\n",
        "  tweet = demojize(tweet) # replace emojis\n",
        "  if verbose: print(\"Post Word processing tweet: {}\".format(tweet))\n",
        "\n",
        "  ## Tokenization & Stemming\n",
        "  tokens = tokenize(tweet, keep_alnum=False, keep_stop=False) # tokenize\n",
        "  stemmer = SnowballStemmer(\"english\") # define stemmer\n",
        "  stem = stem_tokens(tokens, stemmer) # stem tokens\n",
        "\n",
        "  return stem"
      ],
      "metadata": {
        "id": "i3A9enasA7yA"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(process_tweet(complex_tweet, verbose=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dZzKEl5BUR3",
        "outputId": "5bb58d68-46e2-42ce-84d5-6e786e9af1b2"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hey', 'look', 'big', 'complex', 'tweet', 'i', 'would', 'glad', 'could', 'not', 'normal', 'check', 'let', 'know', 'nlp', 'fun']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorization"
      ],
      "metadata": {
        "id": "PJvqJh_JBole"
      }
    }
  ]
}