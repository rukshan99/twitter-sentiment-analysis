{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "twitter-sentiment-analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NAXrk19ufOFY",
        "ifZWCqFjiyJ8",
        "3RCgWolepQqu",
        "dC8mJ6JXlTaj",
        "p7TXL1T7lb5z",
        "0sp5cCyRl6vE",
        "POwvRyGloHFQ",
        "l624gn0cooLT",
        "epNmVOx0pcYd",
        "bnPjAWAC2Wm_",
        "OODO9fYb2nyR",
        "gqymZxIj2_gZ",
        "nessGAri3gIt",
        "BYXlhkFD4eUG",
        "81U8dagy6WfO",
        "caR_65ou9qvT",
        "gWYGHkeC_Rxa",
        "WmokdVA6AlOm",
        "PJvqJh_JBole",
        "poEkgYd8PRUu",
        "okzheU1Vnxmw",
        "YA1CpSjPpvHg"
      ],
      "authorship_tag": "ABX9TyMIdTT8RTOFFyEe2XM43EOO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rukshan99/twitter-sentiment-analysis/blob/main/twitter_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connecting with Google Drive to get the data-set"
      ],
      "metadata": {
        "id": "NAXrk19ufOFY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZhLkmUkO59g"
      },
      "source": [
        "# Import PyDrive and associated libraries\n",
        "# This only needs to be done once per notebook\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "# This only needs to be done once per notebook\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download a file based on its file ID.\n",
        "\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "file_id = '1x4XtAigTb-fJ2TQJOCDXzAkVOsToBrCT' # Check your own ID in GDrive\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "\n",
        "# Save file in Colab memory\n",
        "downloaded.GetContentFile('tweet_data.csv')  "
      ],
      "metadata": {
        "id": "4sRUE9JSf7-g"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization"
      ],
      "metadata": {
        "id": "ifZWCqFjiyJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Regex package\n",
        "import re"
      ],
      "metadata": {
        "id": "OaW9_7wNi3F5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handle tweet features"
      ],
      "metadata": {
        "id": "3RCgWolepQqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example tweet\n",
        "tweet = \"RT @rukshaann I love this! ðŸ‘ https://rukshanjayasekara.me #Portfolio #Fun\""
      ],
      "metadata": {
        "id": "LZlmLUhvjTmU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Retweet tag"
      ],
      "metadata": {
        "id": "dC8mJ6JXlTaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling the retweet(RT) tag\n",
        "# It is not required for analysing sentiment\n",
        "def replace_retweet(tweet, default_replace=\"\"):\n",
        "  tweet = re.sub('RT\\s+', default_replace, tweet)\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "Xfd-oGtSjxBV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Processed tweet: {}\".format(replace_retweet(tweet)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wa6j0z-OkYSN",
        "outputId": "bb113534-2d0f-4829-c7af-a749f8b19ebf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed tweet: @rukshaann I love this! ðŸ‘ https://rukshanjayasekara.me #Portfolio #Fun\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### User tags"
      ],
      "metadata": {
        "id": "p7TXL1T7lb5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling user tags(@)\n",
        "# It is not required for analysing sentiment\n",
        "def replace_user(tweet, default_replace=\"user\"):\n",
        "  tweet = re.sub('\\B@\\w+', default_replace, tweet)\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "WLm7VWOFksu_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Processed tweet: {}\".format(replace_user(tweet)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtHO4awOlH61",
        "outputId": "df9c4ed5-e282-453a-f46d-b58ee60f6477"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed tweet: RT user I love this! ðŸ‘ https://rukshanjayasekara.me #Portfolio #Fun\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Emojis"
      ],
      "metadata": {
        "id": "0sp5cCyRl6vE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install and import `emoji` package"
      ],
      "metadata": {
        "id": "md6QJljZnvpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install emoji --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9z7K9Z1gl-z6",
        "outputId": "656de541-eadd-4218-f792-d90da58ceb67"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-1.6.1.tar.gz (170 kB)\n",
            "\u001b[?25l\r\u001b[K     |â–ˆâ–ˆ                              | 10 kB 18.7 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–‰                            | 20 kB 23.0 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                          | 30 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                        | 40 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                      | 51 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                    | 61 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 71 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 81 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 92 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž            | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170 kB 5.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.1-py3-none-any.whl size=169314 sha256=8d12b616387d7eb4450237de7fa9695a546cbdd13f73b3c4f10f2a00619301e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/5f/d3/03d313ddb3c2a1a427bb4690f1621eea60fe6f2a30cc95940f\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import emoji"
      ],
      "metadata": {
        "id": "sTc6DRvnn1D1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace emojis with meaningful text\n",
        "def demojize(tweet):\n",
        "  tweet = emoji.demojize(tweet)\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "bDqfaJq_mSZj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Processed tweet: {}\".format(demojize(tweet)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Xo7n7c3mddZ",
        "outputId": "0e7bc095-e0bf-4634-9162-c70e0785ec57"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed tweet: RT @rukshaann I love this! :thumbs_up: https://rukshanjayasekara.me #Portfolio #Fun\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### URLs"
      ],
      "metadata": {
        "id": "POwvRyGloHFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def replace_url(tweet, default_replace=\"\"):\n",
        "  tweet = re.sub('(http|https):\\/\\/\\S+', default_replace, tweet)\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "VOvb-vhgoLmO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Processed tweet: {}\".format(replace_url(tweet)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfDkTFBLoX_O",
        "outputId": "2419369b-52be-4bc2-aa50-0d788ba1404f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed tweet: RT @rukshaann I love this! ðŸ‘  #Portfolio #Fun\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hashtags"
      ],
      "metadata": {
        "id": "l624gn0cooLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove hashtag symbol(#)\n",
        "def replace_hashtag(tweet, default_replace=\"\"):\n",
        "  tweet = re.sub('#+', default_replace, tweet)\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "ghiatcKroqt9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Processed tweet: {}\".format(replace_hashtag(tweet)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdgrcG2Fov82",
        "outputId": "62f60156-80a7-44af-f096-3821b86d5c64"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed tweet: RT @rukshaann I love this! ðŸ‘ https://rukshanjayasekara.me Portfolio Fun\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handle word features"
      ],
      "metadata": {
        "id": "epNmVOx0pcYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example tweet\n",
        "tweet = \"LOOOOOOOOK at this ... I'd like it so much!\""
      ],
      "metadata": {
        "id": "rPjJfMMl2D1a"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Capitals"
      ],
      "metadata": {
        "id": "bnPjAWAC2Wm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_lowercase(tweet):\n",
        "  tweet = tweet.lower()\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "XdObAbGq2P3h"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Processed tweet: {}\".format(to_lowercase(tweet)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLz0IRs42bmi",
        "outputId": "21d42eb0-aa68-4edc-939c-8ecf1b6aa6fb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed tweet: looooooook at this ... i'd like it so much!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Word repitions"
      ],
      "metadata": {
        "id": "OODO9fYb2nyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word_repetition(tweet):\n",
        "  tweet = re.sub(r'(.)\\1+', r'\\1\\1', tweet)\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "JzwfR06L2sSR"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Processed tweet: {}\".format(word_repetition(tweet)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuCaH1iV2zXp",
        "outputId": "f09be218-b3bf-4ac9-e295-f6681744eecb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed tweet: LOOK at this .. I'd like it so much!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Punctuation repetition"
      ],
      "metadata": {
        "id": "gqymZxIj2_gZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def punct_repetition(tweet, default_replace=\"\"):\n",
        "  tweet = re.sub(r'[\\?\\.\\!]+(?=[\\?\\.\\!])', default_replace, tweet)\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "1C149jMM3EDr"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Processed tweet: {}\".format(punct_repetition(tweet)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rutFMc5H3WcK",
        "outputId": "55fc0a05-aa45-46ea-a84b-f5cd8ec48bc8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed tweet: LOOOOOOOOK at this . I'd like it so much!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Word contraction"
      ],
      "metadata": {
        "id": "nessGAri3gIt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install and import the `contraction` package"
      ],
      "metadata": {
        "id": "biemaPtR3h2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21zPcgTl30B_",
        "outputId": "c0b3c75c-185b-42dd-9826-c9e5f22a538d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.0.58-py2.py3-none-any.whl (8.0 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 321 kB 5.7 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 284 kB 46.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85450 sha256=f3eeada9cddea4e29aeab25826c3bf37e2323536296a98f0a8dceffe3d2208eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.0 contractions-0.0.58 pyahocorasick-1.4.2 textsearch-0.0.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import contractions"
      ],
      "metadata": {
        "id": "JS6LvQxX33Wo"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace contractions with their extended forms\n",
        "def fix_contractions(tweet):\n",
        "  tweet = contractions.fix(tweet)\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "y2EBIZj34Bcg"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Processed tweet: {}\".format(fix_contractions(tweet)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52svUGU24J2X",
        "outputId": "723a42b1-2147-4271-deaf-917cea1ad063"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed tweet: LOOOOOOOOK at this ... I would like it so much!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization"
      ],
      "metadata": {
        "id": "BYXlhkFD4eUG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install and import the `NLTK` package"
      ],
      "metadata": {
        "id": "iLbuEPo24skW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmg8jeOU4lC1",
        "outputId": "d7864d3d-c5b8-4532-c5ff-fd5cd8f48871"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "TUq7oQAd4y5H"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the word_tokenize module from NLTK\n",
        "from nltk.tokenize import word_tokenize\n",
        "# Download the Punkt tokenizer model from NLTK\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrXfFU6f4-L3",
        "outputId": "4b3be29b-54a7-4b71-ea78-57e8b16087fa"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example tweet\n",
        "tweet = \"These are 5 different words!\""
      ],
      "metadata": {
        "id": "v57fBxUG5IBH"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns a tweet as a list of tokens\n",
        "def tokenize(tweet):\n",
        "  tokens = word_tokenize(tweet)\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "A178RI9p5S0v"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(tokenize(tweet)))\n",
        "print(\"Tweet tokens: {}\".format(tokenize(tweet)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PztAfMWe5eFo",
        "outputId": "d21f05e8-ebcb-4925-c011-a529f9b073c9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "Tweet tokens: ['These', 'are', '5', 'different', 'words', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Stopwords"
      ],
      "metadata": {
        "id": "81U8dagy6WfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the string package\n",
        "import string\n",
        "# Import the stopwords module from NLTK\n",
        "from nltk.corpus import stopwords\n",
        "# Download stopwords data from NLTK\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Euw4vLgc6upe",
        "outputId": "61229aee-79f1-4532-ce2a-3ccc7a2ab059"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "# Remove useful stopwords for sentiment analysis\n",
        "stop_words.discard('not')"
      ],
      "metadata": {
        "id": "gdcWrpvy7Etu"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjg5sAJx7KMl",
        "outputId": "70a4c7e4-77a8-4322-fe99-ed6965cf743b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a', 'her', 't', 'is', 'under', 'me', 'his', 'too', 'all', 'our', \"you've\", 'why', 'than', 'over', 'can', 'or', 'm', 'here', 'some', 'such', 'hers', 'the', 'more', 'mightn', 'but', 'wasn', \"should've\", 'on', 'y', 'by', 'wouldn', 'having', 'any', 'didn', \"you'll\", 'my', 'ourselves', 'other', 'who', 'which', 'off', 'be', 'myself', 'had', 'them', 'yourself', 'am', 'needn', 'and', 'for', 'to', 'there', 'through', 'shouldn', 'where', 'isn', 'just', \"wasn't\", \"mustn't\", 'down', 'are', 'whom', 'been', 'both', 'will', 'mustn', 'how', 'these', 'll', 'once', 'out', 'herself', 'nor', 's', 'does', 'have', 've', \"isn't\", 'this', \"it's\", \"doesn't\", 'itself', 'do', 'each', 'doesn', 'an', 'ours', \"needn't\", 'after', \"won't\", 'only', 'its', 'as', 'being', 'if', 'no', 'few', 'in', 'you', 'your', \"mightn't\", 'during', 'before', 'below', 'was', \"she's\", \"hadn't\", \"that'll\", \"wouldn't\", \"you'd\", 'about', 'ain', 'what', 'yours', 'from', \"shouldn't\", 'did', 'between', 'o', \"don't\", 'above', 'themselves', 'hadn', 'they', 'he', 'couldn', 'now', 'own', 'shan', \"couldn't\", 'don', 'while', \"weren't\", 'we', 'doing', 'at', 'him', 'when', 'again', 'theirs', 'into', 'up', 'until', 'then', 'haven', 'most', 'very', 'weren', 'against', 'i', \"didn't\", 'with', 'their', 'should', 'won', \"hasn't\", 'has', 'yourselves', 're', 'were', \"haven't\", 'hasn', 'd', 'further', \"aren't\", 'it', 'she', 'ma', 'because', 'same', \"you're\", 'so', \"shan't\", 'aren', 'those', 'that', 'of', 'himself'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update the `tokenize()` function to handle stopwords, punctuations, alphanumerics etc."
      ],
      "metadata": {
        "id": "KWfmhDc57yvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(tweet,\n",
        "             keep_punct = False,\n",
        "             keep_alnum = False,\n",
        "             keep_stop = False):\n",
        "  \n",
        "  tokens = word_tokenize(tweet)\n",
        "\n",
        "  if not keep_punct:\n",
        "    tokens = [token for token in tokens\n",
        "                  if token not in string.punctuation]\n",
        "\n",
        "  if not keep_alnum:\n",
        "    tokens = [token for token in tokens if token.isalpha()]\n",
        "  \n",
        "  if not keep_stop:\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stop_words.discard('not')\n",
        "    tokens = [token for token in tokens if not token in stop_words]\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "nEpCH9Aa794m"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tweet tokens: {}\".format(tokenize(tweet, \n",
        "                                                keep_punct=True, \n",
        "                                                keep_alnum=True, \n",
        "                                                keep_stop=True)))\n",
        "print(\"Tweet tokens: {}\".format(tokenize(tweet, keep_stop=True)))\n",
        "print(\"Tweet tokens: {}\".format(tokenize(tweet)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qUnqA5n9MY5",
        "outputId": "36c9df2f-1578-419a-939a-0c009da416c2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet tokens: ['These', 'are', '5', 'different', 'words', '!']\n",
            "Tweet tokens: ['These', 'are', 'different', 'words']\n",
            "Tweet tokens: ['These', 'different', 'words']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming"
      ],
      "metadata": {
        "id": "caR_65ou9qvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import different libraries and modules used for stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "metadata": {
        "id": "cwxhJSvP9tsA"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example token list\n",
        "tokens = [\"manager\", \"management\", \"managing\"]"
      ],
      "metadata": {
        "id": "7aHCqwca-DLb"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining different stemmers from NLTK package\n",
        "porter_stemmer = PorterStemmer()\n",
        "lancaster_stemmer = LancasterStemmer()\n",
        "snoball_stemmer = SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "FmQevP7Y-N7z"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns a stemmed token list\n",
        "# Should pass the stemmer a parameter\n",
        "def stem_tokens(tokens, stemmer):\n",
        "  token_list = []\n",
        "  for token in tokens:\n",
        "    token_list.append(stemmer.stem(token))\n",
        "  return token_list"
      ],
      "metadata": {
        "id": "QGc40QIu-Uki"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PorterStemmer\n",
        "print(\"Porter stems: {}\".format(stem_tokens(tokens, porter_stemmer)))\n",
        "# LancasterStemmer\n",
        "print(\"Lancaster stems: {}\".format(stem_tokens(tokens, lancaster_stemmer)))\n",
        "# SnowballStemmer\n",
        "print(\"Snowball stems: {}\".format(stem_tokens(tokens, snoball_stemmer)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3t9evag-otB",
        "outputId": "bc38c48d-4fc9-4a1f-c1bc-d5574b027847"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter stems: ['manag', 'manag', 'manag']\n",
            "Lancaster stems: ['man', 'man', 'man']\n",
            "Snowball stems: ['manag', 'manag', 'manag']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check over-stemming and under-stemming"
      ],
      "metadata": {
        "id": "OLTaxR0U_BS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [\"international\", \"companies\", \"had\", \"interns\"]\n",
        "\n",
        "print(\"Porter stems: {}\".format(stem_tokens(tokens, porter_stemmer)))\n",
        "print(\"Lancaster stems: {}\".format(stem_tokens(tokens, lancaster_stemmer)))\n",
        "print(\"Snowball stems: {}\".format(stem_tokens(tokens, snoball_stemmer)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Er1OxfiL_DIK",
        "outputId": "06e1a50c-c70f-44e0-bd50-c7f17d346041"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter stems: ['intern', 'compani', 'had', 'intern']\n",
            "Lancaster stems: ['intern', 'company', 'had', 'intern']\n",
            "Snowball stems: ['intern', 'compani', 'had', 'intern']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization"
      ],
      "metadata": {
        "id": "gWYGHkeC_Rxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import different libraries and modules used for lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jn_T0x3O_WqC",
        "outputId": "7dc78e41-71e1-4443-dd3c-436c29b5eabb"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example token list\n",
        "tokens = [\"international\", \"companies\", \"had\", \"interns\"]"
      ],
      "metadata": {
        "id": "C5R9aefC_qdz"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part of Speech (POS) tagging\n",
        "word_type = {\"international\": wordnet.ADJ, \n",
        "             \"companies\": wordnet.NOUN, \n",
        "             \"had\": wordnet.VERB, \n",
        "             \"interns\": wordnet.NOUN\n",
        "             }"
      ],
      "metadata": {
        "id": "soEOAjZZ_zzB"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the lemmatizer by using the WordNet module\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "WmxwJ-HW_7NB"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Takes the list of tokens as input and returns a list of lemmatized tokens\n",
        "def lemmatize_tokens(tokens, word_type, lemmatizer):\n",
        "  token_list = []\n",
        "  for token in tokens:\n",
        "    token_list.append(lemmatizer.lemmatize(token, word_type[token]))\n",
        "  return token_list"
      ],
      "metadata": {
        "id": "TvesPZDmAFmC"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tweet lemma: {}\".format(\n",
        "    lemmatize_tokens(tokens, word_type, lemmatizer)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIYhSmdwAMlJ",
        "outputId": "89f1bdac-e21b-4e39-befe-4b0fa2b0a66f"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet lemma: ['international', 'company', 'have', 'intern']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final normalizing function"
      ],
      "metadata": {
        "id": "WmokdVA6AlOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "complex_tweet = r\"\"\"RT @rukshaann : hey looooook, \n",
        "THis is a big and complex TWeet!!! ðŸ‘ ... \n",
        "I'd be glad if you couldn't normalize it! \n",
        "Check https://rukshanjayasekara.me and LET ME KNOW!!! #NLP #Fun\"\"\""
      ],
      "metadata": {
        "id": "l12vtYT0AtZh"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_tweet(tweet, verbose=False):\n",
        "  if verbose: print(\"Initial tweet: {}\".format(tweet))\n",
        "\n",
        "  ## Twitter Features\n",
        "  tweet = replace_retweet(tweet) # replace retweet\n",
        "  tweet = replace_user(tweet, \"\") # replace user tag\n",
        "  tweet = replace_url(tweet) # replace url\n",
        "  tweet = replace_hashtag(tweet) # replace hashtag\n",
        "  if verbose: print(\"Post Twitter processing tweet: {}\".format(tweet))\n",
        "\n",
        "  ## Word Features\n",
        "  tweet = to_lowercase(tweet) # lower case\n",
        "  tweet = fix_contractions(tweet) # replace contractions\n",
        "  tweet = punct_repetition(tweet) # replace punctuation repetition\n",
        "  tweet = word_repetition(tweet) # replace word repetition\n",
        "  tweet = demojize(tweet) # replace emojis\n",
        "  if verbose: print(\"Post Word processing tweet: {}\".format(tweet))\n",
        "\n",
        "  ## Tokenization & Stemming\n",
        "  tokens = tokenize(tweet, keep_alnum=False, keep_stop=False) # tokenize\n",
        "  stemmer = SnowballStemmer(\"english\") # define stemmer\n",
        "  stem = stem_tokens(tokens, stemmer) # stem tokens\n",
        "\n",
        "  return stem"
      ],
      "metadata": {
        "id": "i3A9enasA7yA"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(process_tweet(complex_tweet, verbose=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dZzKEl5BUR3",
        "outputId": "5bb58d68-46e2-42ce-84d5-6e786e9af1b2"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hey', 'look', 'big', 'complex', 'tweet', 'i', 'would', 'glad', 'could', 'not', 'normal', 'check', 'let', 'know', 'nlp', 'fun']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorization"
      ],
      "metadata": {
        "id": "PJvqJh_JBole"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the `Scikit-Learn` package "
      ],
      "metadata": {
        "id": "biHUGqO0MVTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuVJ5hc2MQGk",
        "outputId": "f5c184ad-5cb0-4373-d499-fec69c9edff8"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import `pandas` and `numpy` packages"
      ],
      "metadata": {
        "id": "dkoNuuUfMu_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3yhgacgCMpa7"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Dataframe imported from Google Drive under .csv format\n",
        "df = pd.read_csv(\"tweet_data.csv\")"
      ],
      "metadata": {
        "id": "sx7z4NubM3Cz"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply process_tweet function created in [56] to the entire DataFrame\n",
        "df[\"tokens\"] = df[\"tweet_text\"].apply(process_tweet)\n",
        "\n",
        "# Convert sentiment: 1 for \"positive\" and 0 for \"negative\"\n",
        "df[\"tweet_sentiment\"] = df[\"sentiment\"].apply(lambda i: 1\n",
        "                                              if i == \"positive\" else 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "LYiywEIkNSfC",
        "outputId": "cbecd21a-729d-4ae1-a3dc-a58f7e30f186"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-132f1e88-fe27-44d7-892d-18c0be2e0f9a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>tokens</th>\n",
              "      <th>tweet_sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1956967666</td>\n",
              "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
              "      <td>negative</td>\n",
              "      <td>[layin, n, bed, headach, call]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1956967696</td>\n",
              "      <td>Funeral ceremony...gloomy friday...</td>\n",
              "      <td>negative</td>\n",
              "      <td>[funer, friday]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1956967789</td>\n",
              "      <td>wants to hang out with friends SOON!</td>\n",
              "      <td>positive</td>\n",
              "      <td>[want, hang, friend, soon]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1956968477</td>\n",
              "      <td>Re-pinging @ghostridah14: why didn't you go to...</td>\n",
              "      <td>negative</td>\n",
              "      <td>[not, go, prom, bc, bf, not, like, friend]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1956968636</td>\n",
              "      <td>Hmmm. http://www.djhero.com/ is down</td>\n",
              "      <td>negative</td>\n",
              "      <td>[hmm]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1956969035</td>\n",
              "      <td>@charviray Charlene my love. I miss you</td>\n",
              "      <td>negative</td>\n",
              "      <td>[charlen, love, miss]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1956969172</td>\n",
              "      <td>@kelcouch I'm sorry  at least it's Friday?</td>\n",
              "      <td>negative</td>\n",
              "      <td>[i, sorri, least, friday]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1956969531</td>\n",
              "      <td>Choked on her retainers</td>\n",
              "      <td>negative</td>\n",
              "      <td>[choke, retain]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1956970047</td>\n",
              "      <td>Ugh! I have to beat this stupid song to get to...</td>\n",
              "      <td>negative</td>\n",
              "      <td>[ugh, beat, stupid, song, get, next, rude]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1956970424</td>\n",
              "      <td>@BrodyJenner if u watch the hills in london u ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>[watch, hill, london, realis, tourtur, week, w...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-132f1e88-fe27-44d7-892d-18c0be2e0f9a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-132f1e88-fe27-44d7-892d-18c0be2e0f9a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-132f1e88-fe27-44d7-892d-18c0be2e0f9a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       textID  ... tweet_sentiment\n",
              "0  1956967666  ...               0\n",
              "1  1956967696  ...               0\n",
              "2  1956967789  ...               1\n",
              "3  1956968477  ...               0\n",
              "4  1956968636  ...               0\n",
              "5  1956969035  ...               0\n",
              "6  1956969172  ...               0\n",
              "7  1956969531  ...               0\n",
              "8  1956970047  ...               0\n",
              "9  1956970424  ...               0\n",
              "\n",
              "[10 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "id": "1m3ic6edOlS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert DataFrame to two lists: one for the tweet tokens (X) and one for the tweet sentiment (y)\n",
        "X = df[\"tokens\"].tolist()\n",
        "y = df[\"tweet_sentiment\"].tolist()"
      ],
      "metadata": {
        "id": "piGxfakaOsPa"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "e9Tu3ALiO7Gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Term Frequency â€“ Inverse Document Frequency (TF-IDF)"
      ],
      "metadata": {
        "id": "poEkgYd8PRUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example corpus\n",
        "corpus = [[\"love\", \"nlp\"],\n",
        "          [\"miss\", \"you\"],\n",
        "          [\"hate\", \"hate\", \"hate\", \"love\"],\n",
        "          [\"happy\", \"love\", \"hate\"],\n",
        "          [\"i\", \"lost\", \"my\", \"computer\"],\n",
        "          [\"i\", \"am\", \"so\", \"sad\"]]"
      ],
      "metadata": {
        "id": "Ejh0zYa1PUep"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import TfidfVectorizer from the Scikit-learn Library\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "acuncsqUPkC3"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Used to build the TF-IDF vectorizer with the corpus\n",
        "def fit_tfidf(tweet_corpus):\n",
        "  tf_vect = TfidfVectorizer(preprocessor=lambda x: x,\n",
        "                            tokenizer=lambda x: x)\n",
        "  tf_vect.fit(tweet_corpus)\n",
        "  return tf_vect"
      ],
      "metadata": {
        "id": "3xfsYCmHP1qZ"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the vectorizer on the corpus, and transform the corpus\n",
        "tf_vect = fit_tfidf(corpus)\n",
        "tf_mtx = tf_vect.transform(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28iCThBSQDPH",
        "outputId": "f5485e94-62d5-45ea-f79d-a860c11bb938"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the vectorizer features (matrix columns)\n",
        "ft = tf_vect.get_feature_names()\n",
        "\n",
        "print(\"There are {} features in this corpus\".format(len(ft)))\n",
        "print(ft)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "So1xwA4bQZUm",
        "outputId": "7ecd659d-c064-43a3-eb77-1fe834af592d"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 13 features in this corpus\n",
            "['am', 'computer', 'happy', 'hate', 'i', 'lost', 'love', 'miss', 'my', 'nlp', 'sad', 'so', 'you']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the matrix shape\n",
        "print(tf_mtx.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1T8z-YIQgCn",
        "outputId": "daf94dc1-8b23-4c5a-a6db-5632914212ab"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform a new tweet by using the vectorizer"
      ],
      "metadata": {
        "id": "VvNeNaPPQyIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_tweet = [[\"I\", \"hate\", \"nlp\"]]\n",
        "tf_vect.transform(new_tweet).toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxYUjhSzQxGe",
        "outputId": "f460b4a2-79bd-437f-d212-58c05b281e5b"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.6340862 , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.77326237,\n",
              "        0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis"
      ],
      "metadata": {
        "id": "u5yIWIZaT-g5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Helper function\n",
        "used to plot the confusion matrix for the different models"
      ],
      "metadata": {
        "id": "okzheU1Vnxmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "\n",
        "def plot_confusion(cm):\n",
        "  plt.figure(figsize = (5,5))\n",
        "  sn.heatmap(cm, annot=True, cmap=\"Blues\", fmt='.0f')\n",
        "  plt.xlabel(\"Prediction\")\n",
        "  plt.ylabel(\"True value\")\n",
        "  plt.title(\"Confusion Matrix\")\n",
        "  return sn"
      ],
      "metadata": {
        "id": "FsFYnHw5n-Vx"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train/Test data split"
      ],
      "metadata": {
        "id": "YA1CpSjPpvHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check x and y\n",
        "print(X)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "84okdOI4oLEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the train_test_split function from the Scikit-Learn package\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "6Lk0ul2ioYmQ"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the train_test_split() function to split arrays of X and y into training and testing variables\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    random_state=0,\n",
        "                                                    train_size=0.80)"
      ],
      "metadata": {
        "id": "eeYmQS2ioiJf"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Size of X_train: {}\".format(len(X_train)))\n",
        "print(\"Size of y_train: {}\".format(len(y_train)))\n",
        "print(\"\\n\")\n",
        "print(\"Size of X_test: {}\".format(len(X_test)))\n",
        "print(\"Size of y_test: {}\".format(len(y_test)))\n",
        "print(\"\\n\")\n",
        "print(\"Train proportion: {:.0%}\".format(len(X_train)/\n",
        "                                        (len(X_train)+len(X_test))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_K9eAHXpRmP",
        "outputId": "eea73f76-c8d6-48b7-f1f7-4487ba009e52"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of X_train: 14981\n",
            "Size of y_train: 14981\n",
            "\n",
            "\n",
            "Size of X_test: 3746\n",
            "Size of y_test: 3746\n",
            "\n",
            "\n",
            "Train proportion: 80%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Print random tweets, just to verify everything goes as expected\n",
        "id = random.randint(0,len(X_train))\n",
        "print(\"Train tweet: {}\".format(X_train[id]))\n",
        "print(\"Sentiment: {}\".format(y_train[id]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QD56yhJpXZH",
        "outputId": "70837dcb-20b8-448e-9194-491709b2fc5a"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train tweet: ['great', 'news']\n",
            "Sentiment: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Logistic Regression"
      ],
      "metadata": {
        "id": "sc_bydHqqflu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the LogisticRegression model from Scikit-Learn\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "_9hYk8_CqjYF"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Used to fit a Logistic Regression model on X and y training data\n",
        "def fit_lr(X_train, y_train):\n",
        "  model = LogisticRegression()\n",
        "  model.fit(X_train, y_train)\n",
        "  return model"
      ],
      "metadata": {
        "id": "J2BKB1xvqy_e"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF vectorizer"
      ],
      "metadata": {
        "id": "99gyNRKXrTy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf = fit_tfidf(X_train)\n",
        "\n",
        "# Transform X_train and X_test data by using the vectorizer\n",
        "X_train_tf = tf.transform(X_train)\n",
        "X_test_tf = tf.transform(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blGwiAWirZbV",
        "outputId": "9830b71a-3518-4708-b3fd-c843a969bf3c"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the Logistic Regression model on training data by using the fit_lr function\n",
        "model_lr_tf = fit_lr(X_train_tf, y_train)"
      ],
      "metadata": {
        "id": "kHh8gso6rjal"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Performance metrics"
      ],
      "metadata": {
        "id": "nNZ35qvTryRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the accuracy score and confusion matrix from Scikit-Learn\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "25ExQ-7kr1IN"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the fitted model_lr_tf (TF-IDF) to predict X_test\n",
        "y_pred_lr_tf = model_lr_tf.predict(X_test_tf)"
      ],
      "metadata": {
        "id": "ZZ8o_c2psDxV"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the model accuracy by comparing predictions and real sentiments\n",
        "print(\"LR Model Accuracy: {:.2%}\".format(accuracy_score(y_test, y_pred_lr_tf)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUvDc4kdsG8V",
        "outputId": "63c153cb-1b9d-4238-d888-8d25001a43f4"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR Model Accuracy: 88.57%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the confusion matrix by using the plot_confusion helper function\n",
        "plot_confusion(confusion_matrix(y_test, y_pred_lr_tf))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "lqTDCnFqsMlF",
        "outputId": "e4eacb30-b267-439b-c0a3-5378294868f8"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'seaborn' from '/usr/local/lib/python3.7/dist-packages/seaborn/__init__.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 88
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Pipeline"
      ],
      "metadata": {
        "id": "KRcRMJ9ysq0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweet = \"\"\"RT @rukshaann: this is amazing NLP content! \n",
        "And don't forget to visit http://rukshanjayasekara.me ... Thank You!!! #Twitter #NLP #SentimentAnalysis\"\"\""
      ],
      "metadata": {
        "id": "3tSDPhRTtWVz"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_tweet(tweet):\n",
        "  # Normalization\n",
        "  processed_tweet = process_tweet(tweet)\n",
        "  # Vectorization\n",
        "  transformed_tweet = tf.transform([processed_tweet])\n",
        "  # Sentiment analysis\n",
        "  prediction = model_lr_tf.predict(transformed_tweet)\n",
        "\n",
        "  if prediction == 1:\n",
        "    return \"Prediction is positive sentiment\"\n",
        "  else:\n",
        "    return \"Prediction is negative sentiment\""
      ],
      "metadata": {
        "id": "VNRzoYJrt6qD"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_tweet(tweet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ojvYttE6uKb-",
        "outputId": "795df5ea-f741-4a20-ed2a-fbaa723ef943"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Prediction is positive sentiment'"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    }
  ]
}